---
- name: Configure kubernetes
  # hosts: all
  hosts: localhost
  connection: local
  gather_facts: no
  # become: yes
  vars:
    machine_type: n1-standard-1
    project_id: "docker-spinor72"
    credentials_file: ~/.config/gcloud/docker-ea4939aad790.json
    service_account_email: 347423440406-compute@developer.gserviceaccount.com
    region: "europe-west4"

#  When running Ansible inside a GCE VM you can use the service account credentials from the local metadata server 
# by setting both service_account_email and credentials_file to a blank string

  tasks:
    - name: Download cfssl
      get_url:
        url: https://pkg.cfssl.org/R1.2/cfssl_linux-amd64
        dest: /usr/local/bin/cfssl
        mode: +x
      become: yes
# cfssl version
# Version: 1.2.0

    - name: Download cfssljson
      get_url:
        url: https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
        dest: /usr/local/bin/cfssljson
        mode: +x
      become: yes

    - name: Download kubectl
      get_url:
        url: https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kubectl
        dest: /usr/local/bin/kubectl
        mode: +x
      become: yes
#  kubectl version --client  version 1.10.2

#  Virtual Private Cloud Network
    - name: Create kubernetes Network
      gce_net:
        name: kubernetes-the-hard-way
        mode: custom
        subnet_name: "kubernetes"
        subnet_region: "{{ region }}"
        ipv4_range: '10.240.0.0/24'
        state: "present"

    - name: Create a firewall rule that allows internal communication across all protocols
      gce_net:
        name: kubernetes-the-hard-way
        fwname: "kubernetes-the-hard-way-allow-{{ item |replace(':', '-') }}-internal"
        allowed: "{{ item }}"
        state: "present"
        src_range: ['10.240.0.0/24','10.200.0.0/16']
      with_items:
        - tcp
        - udp
        - icmp

    - name: Create a firewall rule that allows external SSH, ICMP, and HTTPS
      gce_net:
        name: kubernetes-the-hard-way
        fwname: "kubernetes-the-hard-way-allow-{{ item |replace(':', '-') }}-external"
        allowed: "{{ item }}"
        state: "present"
        src_range: ['0.0.0.0/0']
      with_items:
        - tcp:22
        - tcp:6443
        - icmp


    - name: Reserve Kubernetes Public IP Address
      gce_eip:
        name: kubernetes-the-hard-way
        region: "{{ region }}"
        state: present
      register: kube_ip



# Kubernetes VMs

    - name: Create Kubernetes Controllers
      gce:
        name: "controller-{{ item }}"
        machine_type: "n1-standard-1"
        disk_size: 200
        ip_forward: yes
        image: https://www.googleapis.com/compute/v1/projects/ubuntu-os-cloud/global/images/family/ubuntu-1604-lts
        service_account_permissions: compute-rw,storage-ro,service-management,service-control,logging-write,monitoring
        network: kubernetes-the-hard-way
        subnetwork:  kubernetes
        # private-network-ip 10.240.0.1${i}
        zone: "{{ region }}-b"
        tags: kubernetes-the-hard-way,controller
      with_sequence: count=3
      register: kube_controllers

    - name: Save controllers to inventory
      add_host:
        hostname: "{{ item.instance_data[0].name }}"
        groupname: controllers
        ansible_host: "{{ item.instance_data[0].public_ip }}"
        private_ip: "{{ item.instance_data[0].private_ip }}"
      with_items: "{{ kube_controllers.results }}"

    - name: Create Kubernetes Workers
      gce:
        name: "worker-{{ item }}"
        machine_type: "n1-standard-1"
        disk_size: 200
        ip_forward: yes
        image: https://www.googleapis.com/compute/v1/projects/ubuntu-os-cloud/global/images/family/ubuntu-1604-lts
        metadata: '{"pod-cidr":"10.200.{{ item }}.0/24"}'
        service_account_permissions: compute-rw,storage-ro,service-management,service-control,logging-write,monitoring
        network: kubernetes-the-hard-way
        subnetwork:  kubernetes
        # private-network-ip 10.240.0.2${i} 
        zone: "{{ region }}-b"
        tags: kubernetes-the-hard-way,worker
      with_sequence: count=3
      register: kube_workers

    - name: Save Workers to inventory
      add_host:
        hostname: "{{ item.instance_data[0].public_ip }}"
        groupname: workers
        ansible_host: "{{ item.instance_data[0].name }}"
      with_items: "{{ kube_workers.results }}"

    - debug:
        var: kube_ip
        # verbosity: 1

    - debug:
        var: kube_controllers
        # verbosity: 1

    - debug:
        var: kube_controllers
        # verbosity: 1

# Kubernetes certs
    - name: CA csr
      template:
        src: templates/csr.j2
        dest: ca/ca-csr.json
      vars:
        cn: Kubernetes
        o: Kubernetes
        ou: CA

    - name: Create ssl certs
      shell: cfssl gencert -initca ca-csr.json | cfssljson -bare ca
      args:
        chdir: ca
        creates: ca-key.pem


    - name: Admin csr
      template:
        src: templates/csr.j2
        dest: ca/admin-csr.json
      vars:
        cn: admin
        o: system:masters
        ou: Kubernetes The Hard Way

    - name: Create admin cert
      shell: |
        cfssl gencert \
        -ca=ca.pem \
        -ca-key=ca-key.pem \
        -config=ca-config.json \
        -profile=kubernetes \
        admin-csr.json | cfssljson -bare admin
      args:
        chdir: ca
        creates: admin-key.pem

# The Kubelet Client Certificates
# Kubernetes uses a special-purpose authorization mode called Node Authorizer, that specifically authorizes API requests made by Kubelets.
# In order to be authorized by the Node Authorizer, Kubelets must use a credential that identifies them as being in the system:nodes group,
# with a username of system:node:<nodeName>
    - name: The Kubelet Client Certificates
      template:
        src: templates/csr.j2
        dest: "ca/{{ item.instance_data[0].name }}-csr.json"
      vars:
        cn: "system:node:{{ item.instance_data[0].name }}"
        o: system:nodes
        ou: Kubernetes The Hard Way
      with_items: "{{ kube_workers.results }}"

    - name: Create worker cert
      shell: "cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -hostname={{ item.instance_data[0].name }},{{ kube_ip.address }},{{ item.instance_data[0].private_ip }} -profile=kubernetes {{ item.instance_data[0].name }}-csr.json | cfssljson -bare {{ item.instance_data[0].name }}"
      args:
        chdir: ca
        creates: "{{ item.instance_data[0].name }}-key.pem"
      with_items: "{{ kube_workers.results }}"


# Generate the kube-controller-manager client certificate and private key:
    - name: The Controller Manager Client Certificate csr
      template:
        src: templates/csr.j2
        dest: ca/kube-controller-manager-csr.json
      vars:
        cn: "system:kube-controller-manager"
        o: system:kube-controller-manager
        ou: Kubernetes The Hard Way

    - name: Create controller manager client certificate
      shell: "cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager"
      args:
        chdir: ca
        creates: "kube-controller-manager-key.pem"


    - name: Generate the kube-proxy client certificate csr
      template:
        src: templates/csr.j2
        dest: ca/kube-proxy-csr.json
      vars:
        cn: "system:kube-proxy"
        o: system:node-proxier
        ou: Kubernetes The Hard Way

    - name: Generate the kube-proxy client certificate and private key
      shell: "cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy"
      args:
        chdir: ca
        creates: "kube-proxy-key.pem"


    - name: Generate the kube-scheduler client csr
      template:
        src: templates/csr.j2
        dest: ca/kube-scheduler-csr.json
      vars:
        cn: "system:kube-scheduler"
        o: system:kube-scheduler
        ou: Kubernetes The Hard Way

    - name: Generate the kube-scheduler client certificate and private key
      shell: "cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-scheduler-csr.json | cfssljson -bare kube-scheduler"
      args:
        chdir: ca
        creates: "kube-scheduler-key.pem"


    - name: Generate the Kubernetes API Server csr
      template:
        src: templates/csr.j2
        dest: ca/kubernetes-csr.json
      vars:
        cn: "kubernetes"
        o: Kubernetes
        ou: Kubernetes The Hard Way

    - name: Generate the Kubernetes API Server certificate and private key
      shell: "cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes -hostname=10.32.0.1,{% for item in kube_controllers.results %}{{item.instance_data[0].private_ip}},{% endfor %}{{ kube_ip.address }},127.0.0.1,kubernetes.default kubernetes-csr.json | cfssljson -bare kubernetes"
      args:
        chdir: ca
        creates: "kubernetes.pem"

    - name: Generate the service-account csr
      template:
        src: templates/csr.j2
        dest: ca/service-account-csr.json
      vars:
        cn: "service-accounts"
        o: Kubernetes
        ou: Kubernetes The Hard Way

    - name: Generate the service-account certificate and private key
      shell: "cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes service-account-csr.json | cfssljson -bare service-account"
      args:
        chdir: ca
        creates: "service-account-key.pem"

# Distribute the Client and Server Certificates

    - name: Copy the appropriate certificates and private keys to each worker instance
      shell: "gcloud compute  scp ca.pem {{ item.instance_data[0].name }}-key.pem {{ item.instance_data[0].name }}.pem {{ item.instance_data[0].name }}:~/"
      args:
        chdir: ca
      with_items: "{{ kube_workers.results }}"

    - name: Copy the appropriate certificates and private keys to each controller instance
      shell: "gcloud compute  scp ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem service-account-key.pem service-account.pem {{ item.instance_data[0].name }}:~/"
      args:
        chdir: ca
      with_items: "{{ kube_controllers.results }}"

# Generate a kubeconfig file for each worker node

    - name: Set cluster config for workers
      shell: >
        kubectl config set-cluster kubernetes-the-hard-way
        --certificate-authority=ca.pem
        --embed-certs=true
        --server=https://{{ kube_ip.address }}:6443
        --kubeconfig={{ item.instance_data[0].name }}.kubeconfig
      args:
        chdir: ca
      with_items: "{{ kube_workers.results }}"


    - name: Set credentials config for workers
      shell: >
        kubectl config set-credentials system:node:{{ item.instance_data[0].name }}
        --client-certificate={{ item.instance_data[0].name }}.pem
        --client-key={{ item.instance_data[0].name }}-key.pem
        --embed-certs=true
        --kubeconfig={{ item.instance_data[0].name }}.kubeconfig
      args:
        chdir: ca
      with_items: "{{ kube_workers.results }}"

    - name: Set context config for workers
      shell: >
        kubectl config set-context default
        --cluster=kubernetes-the-hard-way
        --user=system:node:{{ item.instance_data[0].name }}
        --kubeconfig={{ item.instance_data[0].name }}.kubeconfig
      args:
        chdir: ca
      with_items: "{{ kube_workers.results }}"

    - name: Set default context config for workers
      shell: kubectl config use-context default --kubeconfig={{ item.instance_data[0].name }}.kubeconfig
      args:
        chdir: ca
      with_items: "{{ kube_workers.results }}"



    - name: Set cluster config for kube-proxy
      shell: >
        kubectl config set-cluster kubernetes-the-hard-way
        --certificate-authority=ca.pem
        --embed-certs=true
        --server=https://{{ kube_ip.address}}:6443
        --kubeconfig=kube-proxy.kubeconfig
      args:
        chdir: ca

    - name: Set credentials config for kube-proxy
      shell: >
        kubectl config set-credentials system:kube-proxy
        --client-certificate=kube-proxy.pem
        --client-key=kube-proxy-key.pem
        --embed-certs=true
        --kubeconfig=kube-proxy.kubeconfig
      args:
        chdir: ca

    - name: Set context config for kube-proxy
      shell: >
        kubectl config set-context default
        --cluster=kubernetes-the-hard-way
        --user=system:kube-proxy
        --kubeconfig=kube-proxy.kubeconfig
      args:
        chdir: ca

    - name: Set default context config for kube-proxy
      shell: kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig
      args:
        chdir: ca



# Generate a kubeconfig file for the kube-controller-manager service
    - name: Set cluster config for kube-controller-manager
      shell: >
        kubectl config set-cluster kubernetes-the-hard-way
        --certificate-authority=ca.pem
        --embed-certs=true
        --server=https://127.0.0.1:6443
        --kubeconfig=kube-controller-manager.kubeconfig
      args:
        chdir: ca

    - name: Set credentials config for kube-controller-manager
      shell: >
        kubectl config set-credentials system:kube-controller-manager
        --client-certificate=kube-controller-manager.pem
        --client-key=kube-controller-manager-key.pem
        --embed-certs=true
        --kubeconfig=kube-controller-manager.kubeconfig
      args:
        chdir: ca

    - name: Set context config for kube-controller-manager
      shell: >
        kubectl config set-context default
        --cluster=kubernetes-the-hard-way
        --user=system:kube-controller-manager
        --kubeconfig=kube-controller-manager.kubeconfig
      args:
        chdir: ca

    - name: Set default context config for kube-controller-manager
      shell: kubectl config use-context default --kubeconfig=kube-controller-manager.kubeconfig
      args:
        chdir: ca


#  Generate a kubeconfig file for the kube-scheduler service:
    - name: Set cluster config for kube-scheduler
      shell: >
        kubectl config set-cluster kubernetes-the-hard-way
        --certificate-authority=ca.pem
        --embed-certs=true
        --server=https://127.0.0.1:6443
        --kubeconfig=kube-scheduler.kubeconfig
      args:
        chdir: ca

    - name: Set credentials config for kube-scheduler
      shell: >
        kubectl config set-credentials system:kube-scheduler
        --client-certificate=kube-scheduler.pem
        --client-key=kube-scheduler-key.pem
        --embed-certs=true
        --kubeconfig=kube-scheduler.kubeconfig
      args:
        chdir: ca

    - name: Set context config for kube-scheduler
      shell: >
        kubectl config set-context default
        --cluster=kubernetes-the-hard-way
        --user=system:kube-scheduler
        --kubeconfig=kube-scheduler.kubeconfig
      args:
        chdir: ca

    - name: Set default context config for kube-scheduler
      shell: kubectl config use-context default --kubeconfig=kube-scheduler.kubeconfig
      args:
        chdir: ca


# Generate a kubeconfig file for the admin user:
    - name: Set cluster config for the admin user
      shell: >
        kubectl config set-cluster kubernetes-the-hard-way
        --certificate-authority=ca.pem
        --embed-certs=true
        --server=https://127.0.0.1:6443
        --kubeconfig=admin.kubeconfig
      args:
        chdir: ca

    - name: Set credentials config for the admin user
      shell: >
        kubectl config set-credentials admin
        --client-certificate=admin.pem
        --client-key=admin-key.pem
        --embed-certs=true
        --kubeconfig=admin.kubeconfig
      args:
        chdir: ca

    - name: Set context config for the admin user
      shell: >
        kubectl config set-context default
        --cluster=kubernetes-the-hard-way
        --user=admin
        --kubeconfig=admin.kubeconfig
      args:
        chdir: ca

    - name: Set default context config for the admin user
      shell: kubectl config use-context default --kubeconfig=admin.kubeconfig
      args:
        chdir: ca


# Distribute the Kubernetes Configuration Files
    - name: Copy the appropriate kubelet and kube-proxy kubeconfig files to each worker instance
      shell: "gcloud compute scp {{ item.instance_data[0].name }}.kubeconfig kube-proxy.kubeconfig {{ item.instance_data[0].name }}:~/"
      with_items: "{{ kube_workers.results }}"
      args:
        chdir: ca

    - name: Copy the appropriate certificates and private keys to each controller instance
      shell: "gcloud compute scp admin.kubeconfig kube-controller-manager.kubeconfig kube-scheduler.kubeconfig {{ item.instance_data[0].name }}:~/"
      with_items: "{{ kube_controllers.results }}"
      args:
        chdir: ca

#
# Generating the Data Encryption Config and Key
#

    - name: Generate an encryption key
      shell: head -c 32 /dev/urandom | base64
      register: encryption_key

    - name: Create the encryption-config.yaml encryption config file
      template:
        src: templates/encryption-config.j2
        dest: encryption-config.yaml
      vars:
        ENCRYPTION_KEY: "{{ encryption_key.stdout }}"

    - name: Copy the appropriate certificates and private keys to each controller instance
      shell: "gcloud compute scp encryption-config.yaml {{ item.instance_data[0].name }}:~/"
      with_items: "{{ kube_controllers.results }}"

#
# Bootstrapping the etcd Cluster
# https://github.com/kelseyhightower/kubernetes-the-hard-way/blob/master/docs/07-bootstrapping-etcd.md
#
- name: Bootstrapping the etcd Cluster
  hosts: controllers
  vars:
    ansible_ssh_private_key_file: /home/raa/.ssh/google_compute_engine
    # ansible_user: 
  tasks:
    - name: Download the official etcd release binaries from the coreos/etcd GitHub project
      unarchive:
        src: https://github.com/coreos/etcd/releases/download/v3.3.5/etcd-v3.3.5-linux-amd64.tar.gz
        dest: /tmp/
        remote_src: yes

    - name: Extract and install the etcd server and the etcdctl command line utility
      copy:
        src: "/tmp/etcd-v3.3.5-linux-amd64/{{ item }}"
        dest: /usr/local/bin/
        remote_src: yes
      become: yes
      with_items:
        - etcd
        - etcdctl

    - name: Configure the etcd Server dirs
      become: yes
      file:
        path: "{{ item }}"
        state: directory
      with_items:
        - /var/lib/etcd
        - /etc/etcd/

    - name: Configure the etcd Server
      copy:
        src: "{{ item }}"
        dest: /etc/etcd/
        remote_src: yes
      become: yes
      with_items:
        - ca.pem
        - kubernetes-key.pem
        - kubernetes.pem

    - debug:
        var: hostvars[groups['controllers'][0]]


    - name: Create the etcd.service systemd unit file
      template:
        src: templates/etcd.service.j2
        dest: /etc/systemd/system/etcd.service
      become: yes
      vars:
        INTERNAL_IP: "{{ private_ip }}"
        ETCD_NAME: "{{ inventory_hostname }}"
        CONTROLLERS: "{{ groups['controllers'] }}"
# hostvars[groups['controllers'][0]]

    - name: Start the etcd Server
      become: yes
      systemd:
        name: etcd
        state: started
        enabled: True

# Provision the Kubernetes Control Plane
    - name: Download and Install the Kubernetes Controller Binaries
      get_url:
        url: "https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/{{ item }}"
        dest: /usr/local/bin/
        mode: +x
      become: yes
      with_items:
        - kube-apiserver
        - kube-controller-manager
        - kube-scheduler
        - kubectl

    - name: Configure the kubernetes Server dirs
      become: yes
      file:
        path: "{{ item }}"
        state: directory
      with_items:
        - /var/lib/kubernetes

    - name: Configure the kubernetes Server
      copy:
        src: "{{ item }}"
        dest: /var/lib/kubernetes/
        remote_src: yes
      become: yes
      with_items:
        - ca.pem
        - ca-key.pem
        - kubernetes.pem
        - kubernetes-key.pem
        - service-account-key.pem
        - service-account.pem
        - encryption-config.yaml

# Configure the Kubernetes Controller Manager
    - name: Configure the Kubernetes Controller Manager
      copy:
        src: "{{ item }}"
        dest: /var/lib/kubernetes/
        remote_src: yes
      become: yes
      with_items:
        - kube-controller-manager.kubeconfig

